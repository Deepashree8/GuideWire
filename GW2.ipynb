{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ChQVVQmq0Fz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_metrics = pd.read_csv(\"kubernetes_performance_metrics_dataset[1].csv\")\n",
        "df_resources = pd.read_csv(\"kubernetes_resource_allocation_dataset[1].csv\")\n",
        "print(\"Columns in df_metrics:\", df_metrics.columns)\n",
        "print(\"Columns in df_resources:\", df_resources.columns)"
      ],
      "metadata": {
        "id": "v8mew4lUq2PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Performance Metrics Dataset:\")\n",
        "print(df_metrics.info())\n",
        "print(df_metrics.head())"
      ],
      "metadata": {
        "id": "PW00dyIMJT0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Resource Allocation Dataset:\")\n",
        "print(df_resources.info())\n",
        "print(df_resources.head())"
      ],
      "metadata": {
        "id": "VakL1lcrJVau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Missing Values in Performance Metrics Dataset:\")\n",
        "print(df_metrics.isnull().sum())\n",
        "\n",
        "print(\"\\nMissing Values in Resource Allocation Dataset:\")\n",
        "print(df_resources.isnull().sum())"
      ],
      "metadata": {
        "id": "URVnS9pMJVvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Summary Statistics (Performance Metrics Dataset):\")\n",
        "print(df_metrics.describe())\n",
        "\n",
        "print(\"\\nSummary Statistics (Resource Allocation Dataset):\")\n",
        "print(df_resources.describe())"
      ],
      "metadata": {
        "id": "GM-aNvosJVyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_columns = ['pod_name', 'namespace']\n",
        "df = pd.merge(df_metrics, df_resources, on=common_columns, how='inner')"
      ],
      "metadata": {
        "id": "9jBUTA7HJMvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert timestamp to datetime & sort\n",
        "if 'timestamp' in df.columns:\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df.sort_values(by='timestamp', inplace=True)\n",
        "else:\n",
        "    print(\"Warning: No 'timestamp' column found!\")"
      ],
      "metadata": {
        "id": "qk5FDoQbJO0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "# Use df instead of df_metrics\n",
        "sns.histplot(df['cpu_usage'], kde=True, bins=30, color='blue')\n",
        "plt.title(\"CPU Usage Distribution\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "# Use df instead of df_metrics\n",
        "sns.histplot(df['memory_usage'], kde=True, bins=30, color='red')\n",
        "plt.title(\"Memory Usage Distribution\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7FTLhlxSJc7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "# Exclude non-numeric columns before calculating correlations\n",
        "numeric_df = df_metrics.select_dtypes(include=np.number)\n",
        "sns.heatmap(numeric_df.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Correlation Heatmap (Performance Metrics Dataset)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DR8agXnkJgHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols = ['cpu_usage', 'memory_usage', 'network_bandwidth_usage']\n",
        "df_num = df[num_cols]"
      ],
      "metadata": {
        "id": "-MqBUVsvJhjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df_num)\n",
        "df_scaled = pd.DataFrame(df_scaled, columns=num_cols, index=df.index)\n",
        "\n"
      ],
      "metadata": {
        "id": "a0PIbF2FshZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **Anomaly Detection using One-Class SVM**\n",
        "svm_model = OneClassSVM(kernel='rbf', nu=0.05, gamma='scale')\n",
        "df['anomaly_svm'] = svm_model.fit_predict(df_scaled)"
      ],
      "metadata": {
        "id": "ZuQxYn3rJqA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **Anomaly Detection using Isolation Forest**\n",
        "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
        "df['anomaly_iso'] = iso_forest.fit_predict(df_scaled)"
      ],
      "metadata": {
        "id": "x0r76P7RJsz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert -1 (anomaly) and 1 (normal) to 0/1\n",
        "df['anomaly_svm'] = df['anomaly_svm'].apply(lambda x: 1 if x == -1 else 0)\n",
        "df['anomaly_iso'] = df['anomaly_iso'].apply(lambda x: 1 if x == -1 else 0)"
      ],
      "metadata": {
        "id": "3q6OdRaUJt4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **Plot CPU Usage Anomalies**\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.scatterplot(x=df.index, y=df['cpu_usage'], hue=df['anomaly_svm'], palette={0:'blue', 1:'red'})\n",
        "plt.title('CPU Usage Anomalies (SVM)')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.scatterplot(x=df.index, y=df['cpu_usage'], hue=df['anomaly_iso'], palette={0:'blue', 1:'red'})\n",
        "plt.title('CPU Usage Anomalies (Isolation Forest)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EXEFOe-Js2dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print anomaly summary\n",
        "print(\"SVM Anomaly Distribution:\")\n",
        "print(df['anomaly_svm'].value_counts())\n",
        "print(\"\\nIsolation Forest Anomaly Distribution:\")\n",
        "print(df['anomaly_iso'].value_counts())"
      ],
      "metadata": {
        "id": "JrqV2oyMs_G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add anomaly scores as new features\n",
        "df['anomaly_score'] = df['anomaly_svm'] + df['anomaly_iso']"
      ],
      "metadata": {
        "id": "5IS99vHHsjAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **LSTM Model for Forecasting**\n",
        "features = num_cols + ['anomaly_score']\n",
        "df_lstm = df[['timestamp'] + features].set_index('timestamp')\n",
        "\n",
        "# Normalize data using MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = scaler.fit_transform(df_lstm)\n",
        "df_scaled = pd.DataFrame(df_scaled, columns=features, index=df_lstm.index)\n"
      ],
      "metadata": {
        "id": "moKIGMCyJ4gU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create sequences\n",
        "def create_sequences(data, seq_length=50):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Create sequences\n",
        "seq_length = 50\n",
        "X, y = create_sequences(df_scaled.values, seq_length)\n",
        "\n",
        "# Split into train & test sets\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Reshape for LSTM input\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], len(features)))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], len(features)))"
      ],
      "metadata": {
        "id": "LlFkDRG3slk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **Optimized LSTM Model**\n",
        "model = Sequential()\n",
        "\n",
        "# First LSTM layer\n",
        "model.add(LSTM(units=256, activation='relu', return_sequences=True, input_shape=(seq_length, len(features))))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Second LSTM layer\n",
        "model.add(LSTM(units=64, activation='relu', return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Third LSTM layer\n",
        "model.add(LSTM(units=48, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(len(features)))\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n"
      ],
      "metadata": {
        "id": "O4ynGwN1tDOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "FjCh4rmTKKxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['loss'], label='Training Loss', marker='o', linestyle='-')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', marker='o', linestyle='-')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training & Validation Loss Curve')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LtH0a7vYKLIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **Make Predictions**\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# **Inverse Transform Predictions**\n",
        "y_test_inv = scaler.inverse_transform(np.hstack((y_test, np.zeros((y_test.shape[0], df_scaled.shape[1] - y_test.shape[1])))))[:, :len(num_cols)]\n",
        "y_pred_inv = scaler.inverse_transform(np.hstack((y_pred, np.zeros((y_pred.shape[0], df_scaled.shape[1] - y_pred.shape[1])))))[:, :len(num_cols)]\n",
        "\n",
        "# **Plot Actual vs Predicted Values**\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(df_lstm.index[train_size+seq_length:], y_test_inv[:, 0], label=\"Actual CPU Usage\", color='blue')\n",
        "plt.plot(df_lstm.index[train_size+seq_length:], y_pred_inv[:, 0], label=\"Predicted CPU Usage\", color='red')\n",
        "plt.xlabel(\"Timestamp\", fontsize=12)\n",
        "plt.ylabel(\"CPU Usage\", fontsize=12)\n",
        "plt.xticks(rotation=45, fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.title(\"Actual vs Predicted CPU Usage\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mEz6Kd2MKX-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Assume X_train was used for training (2927, 50, 4)\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit on all 4 features\n",
        "X_train_reshaped = X_train.reshape(-1, 4)  # Reshape to (total_samples, 4)\n",
        "scaler.fit(X_train_reshaped)\n",
        "\n",
        "# Save the updated scaler\n",
        "import joblib\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n"
      ],
      "metadata": {
        "id": "ET44ed7ZT-y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import joblib\n",
        "from keras.models import load_model\n",
        "import keras.losses\n",
        "\n",
        "# Load models\n",
        "svm_model = joblib.load(\"svm_model.pkl\")\n",
        "iso_forest = joblib.load(\"isolation_forest.pkl\")\n",
        "scaler = joblib.load(\"scaler.pkl\")  # Make sure this scaler was fitted on 4 features\n",
        "lstm_model = load_model(\"model.h5\", custom_objects={\"mse\": keras.losses.MeanSquaredError()})\n",
        "\n",
        "# Take input\n",
        "cpu_usage = float(input(\"Enter CPU Usage (0 to 1): \"))\n",
        "memory_usage = float(input(\"Enter Memory Usage (0 to 1): \"))\n",
        "network_usage = float(input(\"Enter Network Usage (0 to 1): \"))\n",
        "\n",
        "# Add a placeholder for the missing 4th feature (set to 0 for now)\n",
        "dummy_feature = np.zeros((1, 1))  # Adjust if you know what this should be\n",
        "input_data = np.array([[cpu_usage, memory_usage, network_usage]])\n",
        "input_data_expanded = np.hstack((input_data, dummy_feature))  # Now it has 4 features\n",
        "\n",
        "# Scale input\n",
        "input_data_scaled = scaler.transform(input_data_expanded)\n",
        "\n",
        "# Reshape to match LSTM input shape (batch_size=1, time_steps=1, features=4)\n",
        "input_data_reshaped = np.reshape(input_data_scaled, (1, 1, input_data_scaled.shape[1]))\n",
        "\n",
        "# Make prediction\n",
        "predicted_usage = lstm_model.predict(input_data_reshaped)\n",
        "\n",
        "# Inverse transform the first 3 features\n",
        "predicted_usage_trimmed = predicted_usage[:, :3]\n",
        "# Add a placeholder for the missing feature (assuming it's 0 or mean)\n",
        "missing_feature = np.zeros((predicted_usage_trimmed.shape[0], 1))  # Shape: (1,1)\n",
        "predicted_usage_fixed = np.hstack([predicted_usage_trimmed, missing_feature])  # Shape: (1,4)\n",
        "\n",
        "# Now apply inverse transform\n",
        "predicted_usage_inv = scaler.inverse_transform(predicted_usage_fixed)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nPredicted Future Resource Usage:\")\n",
        "print(f\"CPU Usage: {predicted_usage_inv[0][0]:.4f}\")\n",
        "print(f\"Memory Usage: {predicted_usage_inv[0][1]:.4f}\")\n",
        "print(f\"Network Usage: {predicted_usage_inv[0][2]:.4f}\")\n"
      ],
      "metadata": {
        "id": "L3EXM_dOT01P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JlO22IzbUBrr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}